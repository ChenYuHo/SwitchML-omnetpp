[General]
network = TwoLayers
simtime-resolution = ps
**.switch_ports = 16
**.max_jobs_to_submit = 2
**.file = "60_job.csv"
**.num_slots = 512
**.MTU = 9000
**.num_gpus = 8
**.collective_scheduler.typename = "None"
**.datarate = 100Gbps
**.delay = 1us
**.job_placement = "random_multiracks"
#include partition.ini

[AllConfigs]
**.collective_scheduler.typename = ${CollectiveScheduler="None", "FifoExclusive", "ByteScheduler", "SincroniaExclusive", "Sincronia", "DeficitRoundRobinExclusive", "DeficitRoundRobin"}

[TwoJob]
**.switch_ports = 3
**.file = "two_jobs.csv"
**.job_placement = "two_jobs"
**.tor_core_channel.typename = "Ideal"
**.delay = 1us

[Exp60Jobs]
extends = AllConfigs
result-dir = tensorkey2/
**.job_placement = ${JobPlacement="random_singlerack", "random_multiracks", "random_distributed", "random"}
**.submit_all_when_start = ${SubmitAllAtStart=true, false}
**.switch_ports = 10
**.num_gpus = 4
**.gpu_scale_factor = 2
**.max_jobs_to_submit = 99
**job_dispatcher**.cmdenv-log-level = debug
**.cmdenv-log-level = off


[TestCollectiveScheduler]
**.num_slots = 5
**.num_updates = 1000
**.num_gpus = 4
**.MTU = 1500
**.file = "1.csv"
**.switch_ports = 3
**.max_jobs_to_submit = 5

[TestFifoExclusive]
extends = TestCollectiveScheduler
**.collective_scheduler.typename = "FifoExclusive"

[TestByteScheduler]
# should see:
# layer 2 chunk 1
# layer 0 chunk 1
# layer 0 chunk 2
# layer 1 chunk 1
# layer 1 chunk 2
# layer 2 chunk 2
extends = TestCollectiveScheduler
**.custom_model = true
# **.custom_model_sizes = "100000,10000,1000000"
# **.custom_fp_times = "2,300000,4"
# **.custom_bp_times = "70000,8,9"
# **.custom_wu_times = "3,7,9000000"
**.custom_model_sizes = "100,100,100"
**.custom_fp_times = "2,3,4"
**.custom_bp_times = "7,8,9"
**.custom_wu_times = "3,7,9"
**.custom_iters = 3
**.collective_scheduler.typename = "ByteScheduler"
**.chunk_size = 90
**.max_jobs_to_submit = 1
**.collective_scheduler.cmdenv-log-level = debug
**Rank0**.cmdenv-log-level = debug
**.cmdenv-log-level = off

[TestByteScheduler_NoSimPkt]
extends = TestByteScheduler
**.packet_simulation = false
**.worker**.cmdenv-log-level = debug


[TestSincronia]
extends = TestCollectiveScheduler
#**.custom_model = true
#**.custom_model_sizes = "2621440,1321440,4321440"
#**.custom_iters = 3
**.switch_ports = 4
**.num_gpus = 4
**.gpu_scale_factor = 2
**.submit_all_when_start = true
**.collective_scheduler.typename = "Sincronia"
#**.chunk_size = 2621440
**.file = "60_job.csv"
**.max_jobs_to_submit = 5
**.collective_scheduler.cmdenv-log-level = debug
**.job_dispatcher.cmdenv-log-level = debug
**Rank0**.cmdenv-log-level = debug
**.cmdenv-log-level = off


[TestDRR]
extends = TestCollectiveScheduler
#**.custom_model = true
#**.custom_model_sizes = "2621440,1321440,4321440"
#**.custom_iters = 3
**.switch_ports = 4
**.num_gpus = 4
**.gpu_scale_factor = 2
**.submit_all_when_start = true
**.collective_scheduler.typename = "DeficitRoundRobin"
#**.chunk_size = 2621440
**.file = "60_job.csv"
**.max_jobs_to_submit = 5
**.collective_scheduler.cmdenv-log-level = debug
**.job_dispatcher.cmdenv-log-level = debug
**Rank0**.cmdenv-log-level = debug
**.cmdenv-log-level = off

[TestMultiRackFallback]
**.job_placement = "random_multiracks_fallback"


[TestEvLog]
**.num_slots = 2
**.num_updates = 10
**.num_gpus = 4
**.MTU = 1500
**.file = "1.csv"
**.switch_ports = 2

[Single]
result-dir = single/
**.job_submitter.typename = "NJobSubmitter"
**.delay = 0s
**.switch_ports = 3
**.num_jobs = 1
**.custom_model = true
**.custom_model_sizes = "2,2,2"
**.custom_fp_times = "1000000000000,1000000000000,1000000000000" # 1s
**.custom_bp_times = "1000000000000,1000000000000,1000000000000"
**.custom_wu_times = "1000000000000,1000000000000,1000000000000"
**.custom_iters = 1
**.num_slots = 1
**.MTU = 25000000000
**.chunk_size = 1
**.num_updates = 1
**.collective_scheduler.typename = ${CollectiveScheduler="None", "FifoExclusive", "ByteScheduler", "Sincronia", "DeficitRoundRobin"}

[Exp15Jobs]
extends = AllConfigs
result-dir = 15_jobs/
cmdenv-status-frequency = 60s
**.job_submitter.typename = "NJobSubmitter"
**.num_jobs = 15
**.iters = 3
**.num_gpus_per_job = 4
**.model = ${model="alexnet", "vgg19", "resnet50", "bert"}
**.switch_ports = 4
**.num_gpus = 5
**.job_placement = ${JobPlacement="random_singlerack_fallback", "random_multiracks_fallback"}
**.cmdenv-log-level = off

[Two]
result-dir = two_workers/
**.file = "two_workers.csv"
#**.job_submitter.typename = "NJobSubmitter"
**.delay = 0s
**.switch_ports = 2
#**.num_jobs = 8
#**.num_gpus_per_job = 2
**.num_gpus = 8
#**.custom_model = true
#**.custom_model_sizes = "2,2,2"
#**.custom_fp_times = "1000000000000,1000000000000,1000000000000" # 1s
#**.custom_bp_times = "1000000000000,1000000000000,1000000000000"
#**.custom_wu_times = "1000000000000,1000000000000,1000000000000"
#**.custom_iters = 1
**.print_only_rank0 = true
#**.num_slots = 1
#**.MTU = 25000000000
#**.chunk_size = 1
#**.num_updates = 1
**.job_placement = "random_multiracks"
**.iters = 3
#**.model = ${model="alexnet", "vgg19", "resnet50", "bert"}
**.collective_scheduler.typename = ${CollectiveScheduler="None", "FifoExclusive", "ByteScheduler", "Sincronia", "SincroniaExclusive", "DeficitRoundRobin"}


[Exp300Jobs]
extends = AllConfigs
result-dir = 300_jobs/
cmdenv-status-frequency = 60s
**.job_submitter.typename = "NJobSubmitter"
**.num_jobs = 300
**.iters = 5
**.num_gpus_per_job = 8
**.model = ${model="alexnet", "vgg11", "vgg16", "vgg19", "resnet50", "resnet101", "resnet152", "inception", "googlenet", "bert"}
**.switch_ports = 16
**.num_gpus = 10
**.job_placement = ${JobPlacement="random_singlerack_fallback", "random_multiracks_fallback", "random_distributed_fallback", "random"}
**.cmdenv-log-level = off

[TestCongestion]
**.delay = 1us
seed-set = 2
**.MTU = 9000
#**.num_updates = ${nu=64, 256}
**.datarate = 100Gbps
**.num_slots = 512
**.switch_ports = 3
**.num_gpus = 16
**.file = "96.csv"
**.max_jobs_to_submit = 48
**.job_placement = "random_multiracks"
**.custom_model = true
**.custom_model_sizes = "26214400"
**.custom_iters = 1
#**.result-recording-modes = -
**.retransmission_timeout = 10ms


[Bert300]
extends = AllConfigs
result-dir = bert_300/
cmdenv-status-frequency = 60s
**.job_placement = ${JobPlacement="random_singlerack_fallback", "random_multiracks_fallback", "random_distributed_fallback", "random"}
**.switch_ports = 16
**.submit_all_when_start = true
**.num_gpus = 10
**.file = "300_bert.csv"
**.max_jobs_to_submit = 9999
**.job_dispatcher**.cmdenv-log-level = debug
**.cmdenv-log-level = off

[SwitchMLPaper]
**.tor_core_channel.typename = "Ideal"
**.delay = 1us
**.MTU = 0
**.num_updates = ${nu=64, 256}
**.datarate = ${bw=10, 100}Gbps
**.num_slots = ${ns=128, 512 ! bw}
**.switch_ports = 3
**.num_gpus = 2
**.file = "1.csv"
**.max_jobs_to_submit = 1
**.job_placement = "random_singlerack"
**.custom_model = true
**.custom_model_sizes = "26214400"
**.custom_iters = 1
#**.result-recording-modes = -
**.retransmission_timeout = 10ms
#**.tor_worker_channel**.result-recording-modes = all,+vector,+histogram
#**.job_dispatcher.**.result-recording-modes = -
#**.workers[*].**.result-recording-modes = -
#**.workers[*].**.result-recording-modes = -

[SwitchMLPaper_NoSimPkt]
extends = SwitchMLPaper
**.packet_simulation = false
**.collective_scheduler.cmdenv-log-level = debug
**Rank0**.cmdenv-log-level = debug
**.worker**.cmdenv-log-level = debug
**.job_dispatcher**.cmdenv-log-level = debug
**.cmdenv-log-level = off
